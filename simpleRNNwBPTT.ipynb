{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJTGQQ4v4VaZ",
        "outputId": "88581a54-e485-4ad5-992d-41dd821b1a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss before update: 4.42928666078666\n",
            "Grad W_xh:\n",
            " [[ 0.14190064 -0.03085566 -0.16590029]\n",
            " [ 0.2048493  -0.25288768 -0.26299734]\n",
            " [ 0.22665322 -0.39487468 -0.23698683]\n",
            " [-0.19705645  0.26551573  0.22413435]]\n",
            "Grad W_hh:\n",
            " [[ 0.00652463 -0.00843032  0.05760394  0.0185553 ]\n",
            " [ 0.01580351 -0.00631523  0.06006751  0.02529782]\n",
            " [ 0.0139037  -0.00806739  0.04922337  0.01920442]\n",
            " [-0.01248669  0.00766473 -0.05047733 -0.01995666]]\n",
            "Grad W_hy:\n",
            " [[ 0.15304402  0.00765917  0.83909957  0.28807495]\n",
            " [ 0.26431672  0.09696962 -0.08838056  0.30032227]]\n",
            "Grad b_h:\n",
            " [ 0.16823351  0.3340134   0.42787586 -0.32430673]\n",
            "Grad b_y:\n",
            " [3.37718054 1.00718345]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rnn_forward(inputs, h0, W_xh, W_hh, W_hy, b_h, b_y):\n",
        "    \"\"\"\n",
        "    Forward pass for a simple RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs : list or array of shape (T, input_dim)\n",
        "        Sequence of input vectors.\n",
        "    h0 : array of shape (hidden_dim,)\n",
        "        Initial hidden state.\n",
        "    W_xh : array of shape (hidden_dim, input_dim)\n",
        "    W_hh : array of shape (hidden_dim, hidden_dim)\n",
        "    W_hy : array of shape (output_dim, hidden_dim)\n",
        "    b_h : array of shape (hidden_dim,)\n",
        "    b_y : array of shape (output_dim,)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    hs : list of hidden states (length T+1, hs[0] = h0)\n",
        "    ys : list of outputs (length T)\n",
        "    caches : list of dict, each dict has intermediate values needed for BPTT\n",
        "    \"\"\"\n",
        "    T = len(inputs)\n",
        "    hs = [h0]\n",
        "    ys = []\n",
        "    caches = []\n",
        "\n",
        "    for t in range(T):\n",
        "        x_t = inputs[t]\n",
        "\n",
        "        # Pre-activation\n",
        "        z_t = np.dot(W_xh, x_t) + np.dot(W_hh, hs[-1]) + b_h\n",
        "        # Hidden state\n",
        "        h_t = np.tanh(z_t)\n",
        "        # Output\n",
        "        y_t = np.dot(W_hy, h_t) + b_y\n",
        "\n",
        "        hs.append(h_t)\n",
        "        ys.append(y_t)\n",
        "\n",
        "        # Store values needed for backprop\n",
        "        cache = {\n",
        "            \"x_t\": x_t,       # current input\n",
        "            \"z_t\": z_t,       # pre-activation of hidden\n",
        "            \"h_t\": h_t,       # current hidden state\n",
        "            \"h_prev\": hs[-2], # previous hidden state\n",
        "        }\n",
        "        caches.append(cache)\n",
        "\n",
        "    return hs, ys, caches\n",
        "\n",
        "def mse_loss(ys, targets):\n",
        "    \"\"\"\n",
        "    Compute the Mean Squared Error (MSE) loss over T time steps.\n",
        "\n",
        "    ys : list of predicted outputs (each shape = (output_dim,))\n",
        "    targets : list of target outputs (same shape as ys)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float\n",
        "    \"\"\"\n",
        "    T = len(ys)\n",
        "    loss = 0.0\n",
        "    for t in range(T):\n",
        "        diff = ys[t] - targets[t]\n",
        "        # 0.5 * sum of squares is convenient for derivatives\n",
        "        loss += 0.5 * np.sum(diff**2)\n",
        "    return loss\n",
        "\n",
        "def rnn_backward(ys, targets, hs, caches, W_xh, W_hh, W_hy, b_h, b_y):\n",
        "    \"\"\"\n",
        "    Backpropagation Through Time for a simple RNN using MSE loss.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ys : list of predicted outputs (length T)\n",
        "    targets : list of target outputs (length T)\n",
        "    hs : list of hidden states (length T+1, hs[0] is h0)\n",
        "    caches : list of dict, each with\n",
        "        'x_t', 'z_t', 'h_t', 'h_prev'\n",
        "    W_xh, W_hh, W_hy, b_h, b_y : RNN parameters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    grads : dict of gradients w.r.t. each parameter\n",
        "    \"\"\"\n",
        "    # Initialize gradients to zero\n",
        "    dW_xh = np.zeros_like(W_xh)\n",
        "    dW_hh = np.zeros_like(W_hh)\n",
        "    dW_hy = np.zeros_like(W_hy)\n",
        "    db_h  = np.zeros_like(b_h)\n",
        "    db_y  = np.zeros_like(b_y)\n",
        "\n",
        "    # We'll need the gradient wrt the hidden state for each time\n",
        "    T = len(ys)\n",
        "    dh_next = np.zeros_like(hs[0])  # shape = (hidden_dim,)\n",
        "\n",
        "    # Backprop through time (from the last time step to the first)\n",
        "    for t in reversed(range(T)):\n",
        "        # 1) Gradient of loss wrt y_t\n",
        "        dy_t = (ys[t] - targets[t])  # shape = (output_dim,)\n",
        "\n",
        "        # 2) Grad wrt W_hy, b_y\n",
        "        dW_hy += np.outer(dy_t, hs[t+1])  # hs[t+1] = h_t\n",
        "        db_y  += dy_t\n",
        "\n",
        "        # 3) Grad wrt h_t, add gradient from future hidden states (dh_next)\n",
        "        dh_t = np.dot(W_hy.T, dy_t) + dh_next  # shape = (hidden_dim,)\n",
        "\n",
        "        # 4) Grad wrt z_t (where z_t is pre-activation)\n",
        "        # h_t = tanh(z_t) => d h_t / d z_t = 1 - h_t^2\n",
        "        h_t = hs[t+1]\n",
        "        dz_t = dh_t * (1 - h_t**2)\n",
        "\n",
        "        # 5) Grad wrt W_xh, W_hh, b_h\n",
        "        x_t = caches[t][\"x_t\"]\n",
        "        h_prev = caches[t][\"h_prev\"]\n",
        "\n",
        "        dW_xh += np.outer(dz_t, x_t)\n",
        "        dW_hh += np.outer(dz_t, h_prev)\n",
        "        db_h  += dz_t\n",
        "\n",
        "        # 6) Grad wrt h_{t-1} -> used for next iteration\n",
        "        dh_next = np.dot(W_hh.T, dz_t)\n",
        "\n",
        "    # Compile gradients in a dictionary\n",
        "    grads = {\n",
        "        \"W_xh\": dW_xh,\n",
        "        \"W_hh\": dW_hh,\n",
        "        \"W_hy\": dW_hy,\n",
        "        \"b_h\": db_h,\n",
        "        \"b_y\": db_y\n",
        "    }\n",
        "    return grads\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # -------------------------------\n",
        "    # 1. Hyperparameters\n",
        "    # -------------------------------\n",
        "    input_dim = 3\n",
        "    hidden_dim = 4\n",
        "    output_dim = 2\n",
        "    T = 5             # sequence length\n",
        "    lr = 0.01         # learning rate\n",
        "\n",
        "    # -------------------------------\n",
        "    # 2. Create synthetic data\n",
        "    # -------------------------------\n",
        "    np.random.seed(42)\n",
        "    inputs = [np.random.randn(input_dim) for _ in range(T)]\n",
        "    targets = [np.random.randn(output_dim) for _ in range(T)]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 3. Initialize parameters\n",
        "    # -------------------------------\n",
        "    W_xh = np.random.randn(hidden_dim, input_dim) * 0.1\n",
        "    W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
        "    W_hy = np.random.randn(output_dim, hidden_dim) * 0.1\n",
        "    b_h  = np.zeros(hidden_dim)\n",
        "    b_y  = np.zeros(output_dim)\n",
        "\n",
        "    # Initial hidden state\n",
        "    h0 = np.zeros(hidden_dim)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 4. Forward pass\n",
        "    # -------------------------------\n",
        "    hs, ys, caches = rnn_forward(inputs, h0, W_xh, W_hh, W_hy, b_h, b_y)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 5. Compute loss\n",
        "    # -------------------------------\n",
        "    loss_value = mse_loss(ys, targets)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 6. Backward pass (BPTT)\n",
        "    # -------------------------------\n",
        "    grads = rnn_backward(ys, targets, hs, caches, W_xh, W_hh, W_hy, b_h, b_y)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7. Parameter update\n",
        "    # -------------------------------\n",
        "    W_xh -= lr * grads[\"W_xh\"]\n",
        "    W_hh -= lr * grads[\"W_hh\"]\n",
        "    W_hy -= lr * grads[\"W_hy\"]\n",
        "    b_h  -= lr * grads[\"b_h\"]\n",
        "    b_y  -= lr * grads[\"b_y\"]\n",
        "\n",
        "    # -------------------------------\n",
        "    # 8. Print results\n",
        "    # -------------------------------\n",
        "    print(\"Loss before update:\", loss_value)\n",
        "    print(\"Grad W_xh:\\n\", grads[\"W_xh\"])\n",
        "    print(\"Grad W_hh:\\n\", grads[\"W_hh\"])\n",
        "    print(\"Grad W_hy:\\n\", grads[\"W_hy\"])\n",
        "    print(\"Grad b_h:\\n\", grads[\"b_h\"])\n",
        "    print(\"Grad b_y:\\n\", grads[\"b_y\"])\n"
      ]
    }
  ]
}